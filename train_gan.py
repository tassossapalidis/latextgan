import os
import tensorflow as tf
import numpy as np
from autoencoder_reg import Encoder, Decoder, load_dataset
from gan import Generator, Discriminator
from eval_ae import decode_sentence
import matplotlib.pyplot as plt

## ALSO NEED AUTOENCODER MODEL
## since this model will be pre-trained, we need to load the weights in
## possibly just pass path to model.save() file

'''
assumptions:
- we have a working generator and discriminator
- generator takes a latent vector and outputs a context vector of
  dimension equal to the context vectors generated by the autoencoder
- discriminator takes a context vector and outputs real number in (0 (fake), 1 (real))

- args: units -- size of hidden context vector (and, for now, also dimension of
                 latent z being fed into generator)
        batch_size
        n_generator_train -- how frequently to train the generator
        epochs -- number of epochs to train for
'''

# call constructors
'''encoder = Encoder(args.units...)
decoder = Decoder(args.units...)
generator = ...
discriminator = ..'''


# make a checkpoint and restore autoencoder weights
# this might have to be saved_model; not sure
#checkpoint.restore_from_checkpoint(...)

max_sentence_len = 40
# maximum number of words in vocabulary
max_vocab = 20000
# number of examples (sentences) to use
#num_train_examples = 200000
num_train_examples = 150000
num_dev_examples = 2000

## create datasets
input_tensor_train, target_tensor_train, tokenizer = load_dataset("train.txt", max_sentence_len, max_vocab, num_train_examples)
'''dev = create_dataset(dev_data, num_dev_examples)
input_tensor_dev = tokenizer.texts_to_sequences(dev)
target_tensor_dev = input_tensor_dev
input_tensor_dev = tf.keras.preprocessing.sequence.pad_sequences(input_tensor_dev, padding='post',
                                                        truncating = 'post', maxlen = max_sentence_len, value=0)
target_tensor_dev = tf.keras.preprocessing.sequence.pad_sequences(target_tensor_dev, padding='post',
                                                        truncating = 'post', maxlen = max_sentence_len, value=0)
'''
## define variables for training
# Number of epochs
EPOCHS = 100
# restore from checkpoint file?
restore_model = True
# define batches
BUFFER_SIZE = len(input_tensor_train)
BATCH_SIZE = 64
steps_per_epoch = len(input_tensor_train)//BATCH_SIZE
# define autoencoder architecture
embedding_dim = 128
#units = 256
units = 512
# n train == 3: model deteriorates to only outputting a single sentence
n_generator_train = 5
# Calculate max_length of the tensors
max_length = target_tensor_train.shape[1]
# calculate vocab size (+1 for zero padding)
vocab_size = len(tokenizer.word_index)+1
# number of dev examples
#num_dev_examples = len(target_tensor_dev)
# define optimizer (Adam)
ae_optimizer = tf.keras.optimizers.Adam()
# before: lr of 0.0005
# hparams from 'improved training of wgans'
optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001, beta_1 = 0.5, beta_2 = 0.9)

## create datasets
train_set = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)
train_set = train_set.batch(BATCH_SIZE, drop_remainder=True)

#dev_set = tf.data.Dataset.from_tensor_slices((input_tensor_dev, target_tensor_dev))
#dev_batch = dev_set.batch(num_dev_examples)

## construct model
encoder = Encoder(BATCH_SIZE, vocab_size, embedding_dim, units, 1)
#decoder = Decoder(vocab_size, embedding_dim, units*2, 1)
decoder = Decoder(vocab_size, embedding_dim, units*2, 1)
generator = Generator(5, units*2)
discriminator = Discriminator(5, units*2)


checkpoint_dir = "./training_ckpt_2"
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt")
checkpoint = tf.train.Checkpoint(optimizer=ae_optimizer,
                                 encoder=encoder,
                                 decoder=decoder)
print(tf.train.latest_checkpoint(checkpoint_dir))
checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))

# real_data shape == fake_data shape (batch_size, units)
def grad_penalty(real_data, fake_data):
    alpha = tf.random.uniform(shape = (BATCH_SIZE, 1), minval = 0, maxval = 1)
    vect = alpha*real_data + (1-alpha)*fake_data
    with tf.GradientTape() as tape:
        # prediction shape: (batch_size, 1)
        tape.watch(vect)
        prediction = discriminator(vect)
    # gradients shape: (batch_size, num_variables) ?
    gradients = tape.gradient(prediction, vect)
    #grad_norm = tf.linalg.norm(gradients, axis = 1)
    #return tf.math.reduce_mean((grad_norm - 1)**2)
    slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=1))
    gradient_penalty = tf.reduce_mean((slopes-1.)**2)
    return gradient_penalty

def discriminator_loss(real_pred, fake_pred):
    ## Wasserstein loss -- no log here
    return -tf.math.reduce_mean(real_pred) + tf.math.reduce_mean(fake_pred)
    #print("fake: {}".format(fake_pred))
    #return -tf.math.reduce_mean(tf.math.log(real_pred)) - tf.math.reduce_mean(tf.math.log(1-fake_pred))

def generator_loss(fake_pred):
    return -tf.math.reduce_mean(fake_pred)
    #print(fake_pred)
    #return -tf.math.reduce_mean(tf.math.log(fake_pred))

def train_step_disc(real_data_batch):
    disc_loss = 0
    with tf.GradientTape() as tape:
        #z = tf.random.normal((args.batch_size, args.units))
        z = tf.random.normal((BATCH_SIZE, units*2))
        hidden = encoder.initialize_hidden_state()
        _, real_vects = encoder(real_data_batch, hidden)
        ## real_vects shape: (batch_size, units)

        #print("real: {}".format(real_vects))
        # discriminator for wgan actually predicts "level of realness" of image
        real_predictions = discriminator(real_vects)
        fake_vects = generator(z)
        #print("fake: {}".format(fake_vects))
        fake_predictions = discriminator(fake_vects)
        disc_loss = discriminator_loss(real_predictions, fake_predictions)
        # 10 has worked best so far here
        disc_loss += 10*grad_penalty(real_vects, fake_vects)
    variables = discriminator.trainable_variables
    gradients = tape.gradient(disc_loss, variables)
    optimizer.apply_gradients(zip(gradients, variables))

    return disc_loss


def train_step_gen():
    gen_loss = 0
    with tf.GradientTape() as tape:
        #z = tf.random.normal((args.batch_size, args.units))
        z = tf.random.normal((BATCH_SIZE, units*2))
        #print(generator(z))
        fake_predictions = discriminator(generator(z))
        gen_loss = generator_loss(fake_predictions)

    variables = generator.trainable_variables
    gradients = tape.gradient(gen_loss, variables)
    optimizer.apply_gradients(zip(gradients, variables))

    return gen_loss

def main():
    optimizer = tf.keras.optimizers.Adam()
    disc_losses = []
    gen_losses = []
    for epoch in range(EPOCHS):
        print("epoch {}".format(epoch+1))
        disc_loss = 0
        gen_loss = 0
        for (i, (x, y)) in enumerate(train_set.take(100)):
            disc_loss += train_step_disc(x)
            # i think this is the way its implemented in the wgan paper
            # training generator every n batches rather than every n epochs
            if (i % n_generator_train) == 0:
                gen_loss += train_step_gen()
        disc_loss /= (i+1) 
        gen_loss /= (np.floor((i+1)/n_generator_train))
        print(disc_loss)
        disc_losses.append(disc_loss)
        print(gen_loss)
        gen_losses.append(gen_loss)

        '''if (epoch % n_generator_train) == 0:
            gen_loss = 0

            # change this. don't actually need data but just want to make sure
            # generator is getting same number of updates as discriminator.
            # also not totally sure if this is the right thing to do
            for i, _ in enumerate(train_set.take(100)):
                gen_loss += train_step_gen()
                #print("gen loss: {}".format(gen_loss))
            gen_loss /= (i+1)
            print("gen loss: {}".format(gen_loss))
            gen_losses.append(gen_loss)'''

        decode_sentence(decoder, generator(tf.random.normal((1, units*2))), tokenizer)
    
    plt.plot(range(100), disc_losses)
    plt.plot(range(0, 100), gen_losses)

    plt.show()

if __name__ == '__main__':
    main()
